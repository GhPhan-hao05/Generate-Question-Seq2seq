{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d169f209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\envs\\machineLearning\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Admin\\envs\\machineLearning\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from sample_squad_data import *\n",
    "# from sample_another_data import *\n",
    "import pickle\n",
    "import time\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273aaf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e48ec",
   "metadata": {
    "code_folding": [
     47
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers = 1,\n",
    "                          dropout= dropout, bidirectional=True, batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, input_emb, lengths, device):\n",
    "        lengths = lengths.cpu()\n",
    "        emb = pack_padded_sequence(input_emb, lengths, batch_first=False, enforce_sorted=False)\n",
    "        self.rnn.flatten_parameters()\n",
    "        outputs, hidden_t = self.rnn(emb)\n",
    "        outputs = pad_packed_sequence(outputs, batch_first=False)\n",
    "        return outputs, hidden_t\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, enc_size, dec_size, n_layers, att_vec_size, bert_model_name, dropout= 0.1): \n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.rnn = nn.GRU(\n",
    "            992, dec_size,\n",
    "            num_layers=n_layers, dropout=dropout,\n",
    "            bidirectional=False, batch_first=False) #dec_size : hidden size của GRU\n",
    "        self.attn = ConcatAttention(enc_size, dec_size, att_vec_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = dec_size\n",
    "\n",
    "    def forward(self, question_ids, hidden, context, init_attn_weighted_context):#context: decinit, init_weight = 0\n",
    "        g_outputs = []\n",
    "        c_outputs = []\n",
    "        copy_gate_outputs = []\n",
    "        current_attn_weighted_context = init_attn_weighted_context\n",
    "        precompute = None\n",
    "        with torch.no_grad():\n",
    "            word_emb = self.bert.embeddings.word_embeddings(question_ids)\n",
    "        emb_t = self.dropout(word_emb)\n",
    "        emb_t = emb_t.squeeze(1)  # emb_t shape: [batch_size, emb_dim] [32, 768]\n",
    "        decoder_rnn_input_t = emb_t # 10,768\n",
    "        decoder_rnn_input_t = torch.cat([emb_t, current_attn_weighted_context], 1).unsqueeze(0) # 768 + weight\n",
    "        output, hidden = self.rnn(decoder_rnn_input_t, hidden) #1, 32, dim----1, 32, dim\n",
    "        output= output.squeeze(0)\n",
    "        current_attn_weighted_context, attn, precompute = self.attn(output, context.transpose(0, 1), precompute)\n",
    "        return output, hidden, attn, current_attn_weighted_context\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, dec_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(dec_size, vocab_size),\n",
    "            nn.Softmax(dim=1))\n",
    "    def forward(self, g_output_t):\n",
    "        return self.generator(g_output_t)\n",
    "    \n",
    "class ConcatAttention(nn.Module):\n",
    "    def __init__(self, context_dim, query_dim, att_dim):\n",
    "        super(ConcatAttention, self).__init__()\n",
    "        self.context_dim = context_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.att_dim = att_dim\n",
    "        self.linear_pre = nn.Linear(context_dim, att_dim, bias=True)\n",
    "        self.linear_q = nn.Linear(query_dim, att_dim, bias=False)\n",
    "        self.linear_v = nn.Linear(att_dim, 1, bias=False)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, input, context, precompute=None):\n",
    "        if precompute is None:\n",
    "            precompute00 = self.linear_pre(context.contiguous().view(-1, context.size(2))) # reshape to (..., hidden_size)\n",
    "            precompute = precompute00.view(context.size(0), context.size(1), -1)\n",
    "        targetT = self.linear_q(input).unsqueeze(1)\n",
    "        tmp10 = precompute + targetT.expand_as(precompute)\n",
    "        tmp20 = self.tanh(tmp10)\n",
    "        energy = self.linear_v(tmp20.view(-1, tmp20.size(2))).view(tmp20.size(0), tmp20.size(1))\n",
    "        score = self.sm(energy)\n",
    "        score_m = score.view(score.size(0), 1, score.size(1))\n",
    "        weightedContext = torch.bmm(score_m, context).squeeze(1)\n",
    "        return weightedContext, score, precompute\n",
    "\n",
    "class DecIniter(nn.Module):\n",
    "    def __init__(self, enc_rnn_size, dec_rnn_size):\n",
    "        super(DecIniter, self).__init__()\n",
    "        self.initer = nn.Linear(\n",
    "            enc_rnn_size,\n",
    "            dec_rnn_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear = nn.Linear(16, enc_rnn_size//2)\n",
    "# kết hợp đầu ra encoder và style embeddinh để đưa vào decoder\n",
    "    def forward(self, enc_list):\n",
    "        enc_list[1] = self.linear(enc_list[1])#[batch, enc/2] batch, 112\n",
    "        x = torch.cat((enc_list[0], enc_list[1]), dim=1)#ini h0\n",
    "        return self.tanh(self.initer(x))  \n",
    "    \n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, embedder, encoder,dec_init, decoder, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.embedder = embedder\n",
    "        self.decIniter = dec_init\n",
    "        self.generator= generator\n",
    "        self.style_emb_mat = nn.Parameter(torch.randn(9,16))\n",
    "    def forward(self, input_ids, iob_ids, is_clue, pos_ids, ner_ids, lengths, style_ids, ques_ids, device, teacher_force):#Sample gốm data, question, style_id\n",
    "        out_length = ques_ids.shape[1]\n",
    "        style_ids = style_ids.cpu()\n",
    "        outputs = torch.zeros(out_length, ques_ids.shape[0] , 30522)#[length, batch, vocab]\n",
    "        emb = self.embedder(input_ids, iob_ids, is_clue, pos_ids, ner_ids) \n",
    "        emb = emb.transpose(0, 1)# len. batch. dim\n",
    "        context, hidden_enc = self.encoder(emb, lengths, device)# shape hidden_enc = [2, batch, hidden]\n",
    "       # context[0] : len, batch, dim\n",
    "        y_style_one_hot = torch.eye(9)[style_ids]\n",
    "        y_style_one_hot = y_style_one_hot.to(device)\n",
    "        style_emb = torch.matmul(y_style_one_hot, self.style_emb_mat)#[batch, 16]\n",
    "        hidden_0 = [hidden_enc[1], style_emb]\n",
    "        init_dec_hidden = self.decIniter(hidden_0).unsqueeze(0)\n",
    "        batch_size = context[0].size(1)  \n",
    "        h_size = (\n",
    "            batch_size,\n",
    "            112 * 2)\n",
    "        init_attn_weighted_context = context[0].data.new(*h_size).zero_()\n",
    "        current_attn_weighted_context=init_attn_weighted_context\n",
    "        hidden = init_dec_hidden\n",
    "        in_dec = ques_ids[: , 0] \n",
    "        for t in range(1, out_length):\n",
    "            out, hidden, attn, current_attn_weighted_context = self.decoder(in_dec,\n",
    "                                                                            hidden, \n",
    "                                                                            context[0],\n",
    "                                                                            current_attn_weighted_context)\n",
    "            \n",
    "            out = self.generator(out) #[batch, vocab]\n",
    "            outputs[t] = out\n",
    "            top1 = out.argmax(1)\n",
    "            in_dec = ques_ids[:, t] if random.random() < teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116d478",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_sz, pos_size, ner_size,  ids_binary_emb_dim=12, out_emb= 300, dropout_rate=0.1):\n",
    "        super(CustomEmbedding, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_sz, 256)\n",
    "        self.iob_embedding = nn.Embedding(3, ids_binary_emb_dim)\n",
    "        self.is_clue = nn.Embedding(2, ids_binary_emb_dim)\n",
    "        self.pos_tag_embedding = nn.Embedding(pos_size, ids_binary_emb_dim)\n",
    "        self.ner_tag_embedding = nn.Embedding(ner_size, ids_binary_emb_dim)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(304, 300)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, iob, is_clue, pos_tag_ids, ner_tag_ids):\n",
    "        word_emb = self.word_embedding(input_ids)\n",
    "        iob_emb = self.iob_embedding(iob)\n",
    "        is_clue = self.is_clue(is_clue)\n",
    "        pos_tag_emb = self.pos_tag_embedding(pos_tag_ids)\n",
    "        ner_tag_emb = self.ner_tag_embedding(ner_tag_ids)\n",
    "        # Concatenate all embeddings\n",
    "        combined_emb = torch.cat((word_emb, iob_emb, is_clue, pos_tag_emb, ner_tag_emb), dim=-1)\n",
    "        combined_emb = self.dropout(combined_emb)\n",
    "        combined_emb = combined_emb.to(torch.float32)\n",
    "        combined_emb = self.linear(combined_emb)\n",
    "        return combined_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f4883",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with open('list_ids_data.pkl', 'rb') as file:\n",
    "    loaded_list = pickle.load(file)\n",
    "with open('token_sents.pkl', 'rb') as file:\n",
    "    token_sents = pickle.load(file)\n",
    "with open('id_big.pkl', 'rb') as file:\n",
    "    id_big = pickle.load(file)\n",
    "with open('filtered_questions.pkl', 'rb') as file:\n",
    "    filtered_questions = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a83fa29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49235"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ef975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = loaded_list\n",
    "style_ids = data_ids[0]\n",
    "pos_ids = data_ids[1]\n",
    "\n",
    "ner_ids = data_ids[2]\n",
    "iob_tag = data_ids[3]\n",
    "is_clue = data_ids[4]\n",
    "iob_ids = [[['I', 'O', 'B'].index(item) for item in iob] for iob in iob_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b14508",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for sent in token_sents:\n",
    "    lengths.append( len(sent))\n",
    "lengths = [int(x) for x in lengths]\n",
    "lengths = torch.Tensor(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #embedding đã được load sẵn từ glove\n",
    "with open('glove_emb_tensor.pkl', 'rb') as file:\n",
    "    glove_emb_tensor1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "952e3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_ids_new = [item for idx, item in enumerate(iob_ids[0:50000]) if [idx] not in id_big]\n",
    "is_clue_new = [item for idx, item in enumerate(is_clue[0:50000]) if [idx] not in id_big]\n",
    "pos_tag_ids_new = [item for idx, item in enumerate(pos_ids[0:50000]) if [idx] not in id_big]\n",
    "ner_tag_ids_new = [item for idx, item in enumerate(ner_ids[0:50000]) if [idx] not in id_big]\n",
    "style_ids  = [item for idx, item in enumerate(style_ids[0:50000]) if [idx] not in id_big]\n",
    "# lọc ra các ids của các câu có độ dài quá lớn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a103410",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, max_length):\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = min(len(seq), max_length)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq[:length])\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4eb3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'bert-base-uncased'\n",
    "vocab_size = 30522  \n",
    "iob_size = 3\n",
    "pos_size = 17\n",
    "ner_size = 19\n",
    "dropout_rate = 0.2\n",
    "max_length = 79\n",
    "enc_size= 224\n",
    "dec_size = 224\n",
    "n_layers = 1\n",
    "att_vec_size = 224\n",
    "input_size = 300\n",
    "hidden_size = 112\n",
    "e =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d899ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = filtered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e57298",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(ques, padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "ques_ids = tokenized_inputs['input_ids']\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset('train.txt')\n",
    "ans_text = ds[0]# answer text\n",
    "tokenized_inputs = tokenizer(ans_text, padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "word_ids = tokenized_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_new = 80\n",
    "iob_ids_new = torch.tensor(pad_sequence(iob_ids_new, max_length_new))\n",
    "is_clue_new = torch.tensor(pad_sequence(is_clue_new, max_length_new))\n",
    "pos_tag_ids_new = torch.tensor(pad_sequence(pos_tag_ids_new, max_length_new))\n",
    "ner_tag_ids_new = torch.tensor(pad_sequence(ner_tag_ids_new, max_length_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4889bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_ids_new = [item for sublist in style_ids for item in sublist] # style_ids_new\n",
    "style_ids_new = torch.tensor(style_ids_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec2a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = CustomEmbedding(vocab_size, pos_size, ner_size, dropout_rate=dropout_rate)#1\n",
    "encoder = Encoder(input_size, hidden_size)#2\n",
    "dec_init = DecIniter(enc_size, dec_size)\n",
    "decoder = Decoder(enc_size=enc_size, \n",
    "                 dec_size= dec_size, n_layers=n_layers, att_vec_size=att_vec_size, bert_model_name=bert_model_name)\n",
    "\n",
    "generator = Generator(dec_size = dec_size, vocab_size=vocab_size)\n",
    "seq2seq = Seq2Seq(embedding_layer, encoder, dec_init, decoder, generator)\n",
    "\n",
    "# model = Seq2Seq(embedding_layer, encoder, dec_init, decoder)\n",
    "# state_dict = torch.load('seq2seq_weight_1.pth')\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36f4f3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, ids, iob, clue, pos, ner, lengths, style, ques): \n",
    "        self.ids = ids\n",
    "        self.iob = iob\n",
    "        self.clue = clue\n",
    "        self.pos = pos\n",
    "        self.ner = ner\n",
    "        self.lengths = lengths\n",
    "        self.style = style\n",
    "        self.ques = ques\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.lengths.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.ids[idx], self.iob[idx], self.clue[idx], \n",
    "                self.pos[idx], self.ner[idx], self.lengths[idx], self.style[idx], self.ques[idx])\n",
    "    \n",
    "\n",
    "dataset = CustomDataset(word_ids, iob_ids_new, is_clue_new, pos_tag_ids_new, ner_tag_ids_new, lengths, style_ids_new, ques_ids)\n",
    "\n",
    "# Create the dataloader with batch size of 32\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4214da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(seq2seq.parameters())\n",
    "optimizer = optim.Adam(params, lr=0.1, weight_decay=0.1)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2ffccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = seq2seq.to(device)\n",
    "# generator=generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5331e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq.train()\n",
    "for epoch in range(1):\n",
    "    print(epoch)\n",
    "    i=0\n",
    "    for batch in dataloader:\n",
    "        if i %100 ==0:\n",
    "            print(i)\n",
    "        seq2seq.zero_grad()\n",
    "        emb, iob, clue, pos, ner, len, style, ques  = batch\n",
    "        emb = emb.to(device)\n",
    "        iob = iob.to(device)\n",
    "        clue = clue.to(device)\n",
    "        pos = pos.to(device)\n",
    "        ner = ner.to(device)\n",
    "        len = len.to(device)\n",
    "        style = style.to(device)\n",
    "        ques = ques.to(device)\n",
    "        out = seq2seq(emb, iob, clue, pos, ner, len, style, ques, device, 0.2)\n",
    "        output_dim = out.shape[-1]\n",
    "        out = out[1:].view(-1, output_dim).to(device)\n",
    "        ques = ques.permute(1, 0)\n",
    "        ques = ques[1:].reshape(-1)\n",
    "        print('out', out.shape)\n",
    "        print('trg', ques.shape)\n",
    "        loss = criterion(out, ques)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cadb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NHƯ TRÊN MODEL KHI TRAIN 1 EPOCH MẤT HƠN 1,5 TIẾNG NHƯNG KHÔNG RA KẾT QUẢ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01e63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c2127c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(seq2seq.state_dict(), 'seq2seq_weight.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0ea8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq.eval().to(device)\n",
    "e = 10\n",
    "out = seq2seq(glove_emb_tensor1[0:e].to(device), iob_ids_new[0:e].to(device),\n",
    "              is_clue_new[0:e].to(device), pos_tag_ids_new[0:e].to(device), ner_tag_ids_new[0:e].to(device), \n",
    "              lengths[0:e].to(device), style_ids_new[0:e].to(device), ques_ids[0:e].to(device), device, 0)\n",
    "\n",
    "out = out.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bd093",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_prd = torch.argmax(out, dim = 2)\n",
    "print(ids_prd.shape)\n",
    "for sent in ids_prd:\n",
    "    tokens = tokenizer.convert_ids_to_tokens(sent)\n",
    "    print('----',tokens, '-----')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a85c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
