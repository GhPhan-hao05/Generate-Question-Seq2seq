{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f3795a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Users\\Admin\\envs\\machineLearning\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\Admin\\envs\\machineLearning\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from sample_squad_data import *\n",
    "# from sample_another_data1 import *\n",
    "import pickle\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3b877d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIAI ĐOẠN CHUẨN BỊ DATA CHO FINE TUNING\n",
    "#CÁC ĐOẠN CODE NÀY ĐÃ ĐƯỢC CHẠY VÀ LƯU VÀO FILE PICKLE (chạy rất lâu)\n",
    "#LẤY STYLE\n",
    "styles_text = []\n",
    "for i in loaded_list[0][0:20000]:\n",
    "    styles_text.append(styles[i[0]])\n",
    "    \n",
    "#LẤY ANSER_TEXT   \n",
    "ft_ans_text = []\n",
    "for i in t[0:20000]:\n",
    "    ft_ans_text.append(i['answer_text'])\n",
    "\n",
    "#LẤY CÂU CHỨA ANSWER\n",
    "ft_ans_sent = []\n",
    "for i in t[0:20000]:\n",
    "    ft_ans_sent.append(i['ans_sent'])\n",
    "\n",
    "#LẤY RA CÁC CLUE\n",
    "ft_clue = []\n",
    "a = 0\n",
    "for i in t[0:20000]:\n",
    "    if a % 1000 == 0:\n",
    "        print(a)\n",
    "    iob = get_iob(i['ans_sent'], i['answer_text'], i['answer_start'])\n",
    "    z = clue_text_ft(i['ans_sent'], i['answer_text'], iob, 7)\n",
    "    ft_clue.append(z)\n",
    "    a+=1\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cffeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fineturning T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "073649f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import spacy\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ad2206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('./Data/ft_style.pkl', 'rb') as file:\n",
    "    styles_text= pickle.load(file)\n",
    "with open('./Data/ft_ans_text.pkl', 'rb') as file:\n",
    "    ft_ans_text= pickle.load(file)\n",
    "with open('./Data/ft_ans_sent.pkl', 'rb') as file:\n",
    "    ft_ans_sent= pickle.load(file)\n",
    "with open('./Data/ft_clue.pkl', 'rb') as file:\n",
    "    ft_clue= pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a497c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id_big.pkl', 'rb') as file:\n",
    "    id_big = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb9535d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = [i for i in id_big  if (i[0] <20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73f022ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loại bỏ những câu có độ dài quá lớn\n",
    "styles_text_new = [item for idx, item in enumerate(styles_text) if [idx] not in id]\n",
    "ft_ans_text_new = [item for idx, item in enumerate(ft_ans_text) if [idx] not in id]\n",
    "ft_ans_sent_new = [item for idx, item in enumerate(ft_ans_sent) if [idx] not in id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89d017ee",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Answer', 'Style', 'Source', 'Output'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tạo data frame\n",
    "df = pd.DataFrame({\n",
    "    'Answer': ft_ans_text_new,\n",
    "    'Style': styles_text_new,\n",
    "    'Source': ft_ans_sent_new\n",
    "})\n",
    " \n",
    "df['Output'] = 'answer: ' + df['Answer'] + ' style: ' + df['Style']\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf31282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df[['Source', 'Output']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a6bac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\envs\\machineLearning\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration,Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "489d38dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0578aadf724ae4b7c208bfbd20c2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\envs\\machineLearning\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# taoh mẫu cho dữ liệu đầu vào: \n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"get anwer style of: {s}\" for s in examples['Source']]\n",
    "    model_inputs = tokenizer(inputs, padding='max_length',max_length=88, truncation=True)\n",
    "    \n",
    "    # Tokenize output (clue + answer)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['Output'], padding='max_length',max_length=90, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fb46483",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "optimizer = torch.optim.Adam(\n",
    "        params=model.parameters(), lr=0.001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b565c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9693fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0999d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\envs\\machineLearning\\Lib\\site-packages\\accelerate\\accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9864' max='9864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9864/9864 41:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.049606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.036835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>0.029482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9864, training_loss=0.06251982886522453, metrics={'train_runtime': 2501.8919, 'train_samples_per_second': 23.652, 'train_steps_per_second': 3.943, 'total_flos': 1376521283174400.0, 'train_loss': 0.06251982886522453, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.001,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Sử dụng Trainer API để huấn luyện\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_train,\n",
    ")\n",
    "\n",
    "# Bắt đầu quá trình huấn luyện\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8115f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: 2 style: how\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\envs\\machineLearning\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"get anwer style of:i have 2 brother\", return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93111293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('T5_ans_clue_style\\\\tokenizer_config.json',\n",
       " 'T5_ans_clue_style\\\\special_tokens_map.json',\n",
       " 'T5_ans_clue_style\\\\spiece.model',\n",
       " 'T5_ans_clue_style\\\\added_tokens.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lưu lại mô hình t5\n",
    "model.save_pretrained('T5_ans_clue_style')\n",
    "tokenizer.save_pretrained('T5_ans_clue_style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9536e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7fa0152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276081c26fc84dd2bf3acd9b0a67e24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a5f81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = T5ForConditionalGeneration.from_pretrained('./T5/T5_ans_clue_style')\n",
    "# t = T5Tokenizer.from_pretrained('./T5/T5_ans_clue_style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ce4473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f17667a8a4449aa2f73d626a117f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564135a38bcb4f099fdcf81ad8f8be53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/HF-Hao/T5_clue_style/commit/4c9db1dfb4ae25e6437d7ce267e5fea8f81d939a', commit_message='Upload tokenizer', commit_description='', oid='4c9db1dfb4ae25e6437d7ce267e5fea8f81d939a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m.push_to_hub(\"T5_clue_style\")\n",
    "# t.push_to_hub(\"T5_clue_style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3abb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\envs\\machineLearning\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\envs\\machineLearning\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: Beam Search style: what\n"
     ]
    }
   ],
   "source": [
    "#load mô hình T5 để sử dụng\n",
    "model_api = T5ForConditionalGeneration.from_pretrained('HF-Hao/T5_clue_style')\n",
    "tokenizer_api = T5Tokenizer.from_pretrained('HF-Hao/T5_clue_style')\n",
    "input_ids = tokenizer_api(\"get anwer style of: Beam Search is a popular technique in generating the best outputs from the model\", return_tensors=\"pt\").input_ids\n",
    "outputs = model_api.generate(input_ids, num_beams=5, num_return_sequences=3)\n",
    "print(tokenizer_api.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba82a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f02fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8bde0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f26c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hàm để lấy clue text\n",
    "def clue_text_ft(context, answer, answer_bio_ids, max_dependency_distance, processed_by_spacy=False):\n",
    "    answer_start = answer_bio_ids.index('B')\n",
    "    try:\n",
    "        answer_end = list(reversed(answer_bio_ids)).index('I')\n",
    "        answer_end = len(answer_bio_ids) - 1 - answer_end\n",
    "    except:\n",
    "        answer_end = answer_start\n",
    "    if not processed_by_spacy:\n",
    "        doc = NLP(context)\n",
    "    else:\n",
    "        doc = context\n",
    "\n",
    "    doc_token_list = [token for token in doc]\n",
    "\n",
    "    idx2token, idx2related, context_tokens = get_all_related(doc, doc_token_list)\n",
    "    clue_flags = [0] * len(doc)\n",
    "    for aid in range(answer_start, answer_end + 1):\n",
    "        sort_related = idx2related[aid]\n",
    "        for tk_id, path in sort_related:\n",
    "            if (tk_id < answer_start or tk_id > answer_end) and len(path) <= max_dependency_distance:\n",
    "                cur_clue = idx2token[tk_id]\n",
    "                if cur_clue.pos_ not in ['ADP', 'DET', 'ADV', 'PUNCT', 'PART']:\n",
    "                    clue_flags[tk_id] = 1\n",
    "    clues = []\n",
    "    i = 0\n",
    "    while i < len(clue_flags):\n",
    "        if clue_flags[i] == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        j = i\n",
    "        while j < len(clue_flags):\n",
    "            if clue_flags[j] == 1:\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "        clue_text = ' '.join(context_tokens[i:j])\n",
    "        clues.append(clue_text)\n",
    "        i = j\n",
    "    clues = ', '.join(clues)\n",
    "    return clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb8eae74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_subtext(text, subtext):\n",
    "    return text.find(subtext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c297009f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (980921373.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def create_input(\"i have 2 brother\", 'how', '2', styles, yes_no_ans, pos_tags_list, ner_tags_list):\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def create_input(sent, st, answ, styles, yes_no_ans, pos_tags_list, ner_tags_list):\n",
    "    style_ids=[]\n",
    "    pos_ids=[]\n",
    "    ner_ids = []\n",
    "    iob_tags = []\n",
    "    is_clue = []\n",
    "    style_ids.append(list(get_style_ids([st], styles, yes_no_ans)))\n",
    "    p, n = get_pos_ner_ids(sent, pos_tags_list, ner_tags_list)\n",
    "    pos_ids.append(p)\n",
    "    ner_ids.append(n)\n",
    "    ans_start= find_subtext(sent, answ)\n",
    "    iob = get_iob(sent, answ, ans_start)\n",
    "    iob_tags.append(iob)\n",
    "    is_clue.append(get_clue_ids_new(sent, answ, iob, 7))\n",
    "    return style_ids, pos_ids, ner_ids, iob_tags, is_clue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32d014ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[6]],\n",
       " [[10, 15, 8, 7]],\n",
       " [[18, 18, 17, 18]],\n",
       " [['O', 'O', 'B', 'O']],\n",
       " [array([1, 1, 0, 1])])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = create_input('i have 2 brother','how', '2', styles, yes_no_ans, pos_tags_list, ner_tags_list)\n",
    "final\n",
    "#cho những đối tượng này qua model để đưa ra câu hỏi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65188bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01376c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ee8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad34acd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f73989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiền xử lý data cho giai đoạn train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2e96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('glove_model.pkl', 'rb') as f:\n",
    "    GLOVE = pickle.load(f)\n",
    "styles=['who', 'where', 'when', 'why', 'which', 'what', 'how','yes-no', 'other']\n",
    "yes_no_ans = ['am', 'is', 'was', 'were', 'are', 'does', 'do', 'did'\n",
    "              , 'have', 'had', 'has', 'could', 'can', 'shall', 'should', 'will', 'would', 'may', 'might']\n",
    "pos_tags_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "ner_tags_list = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL', 'O']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6f329b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_clue_ids_new(context, answer, answer_bio_ids, max_dependency_distance, processed_by_spacy=False):\n",
    "    answer_start = answer_bio_ids.index('B')\n",
    "    try:\n",
    "        answer_end = list(reversed(answer_bio_ids)).index('I')\n",
    "        answer_end = len(answer_bio_ids) - 1 - answer_end\n",
    "    except:\n",
    "        answer_end = answer_start\n",
    "    if not processed_by_spacy:\n",
    "        doc = NLP(context)\n",
    "    else:\n",
    "        doc = context\n",
    "\n",
    "    doc_token_list = [token for token in doc]\n",
    "    # text_str = ' '.join([tk.text for tk in doc])\n",
    "\n",
    "    idx2token, idx2related, context_tokens = get_all_related(doc, doc_token_list) #import\n",
    "    clue_flags = [0] * len(doc)\n",
    "    for aid in range(answer_start, answer_end + 1):\n",
    "        sort_related = idx2related[aid]\n",
    "        for tk_id, path in sort_related:\n",
    "            if (tk_id < answer_start or tk_id > answer_end) and len(path) <= max_dependency_distance:\n",
    "                cur_clue = idx2token[tk_id]\n",
    "                if cur_clue.pos_ not in ['ADP', 'DET', 'ADV', 'PUNCT', 'PART']:\n",
    "                    clue_flags[tk_id] = 1\n",
    "    clues = []\n",
    "    i = 0\n",
    "    while i < len(clue_flags):\n",
    "        if clue_flags[i] == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        j = i\n",
    "        while j < len(clue_flags):\n",
    "            if clue_flags[j] == 1:\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "        clue_text = ' '.join(context_tokens[i:j])\n",
    "        clue_binary_ids = [0] * len(clue_flags)\n",
    "        clue_binary_ids[i:j] = [1] * (j - i)\n",
    "        clues.append({\"clue_text\": clue_text, \"clue_binary_ids\": clue_binary_ids})\n",
    "        i = j\n",
    "    ids = []\n",
    "    for clue in clues:\n",
    "        ids.append(clue['clue_binary_ids'])\n",
    "    if len(ids)== 0:\n",
    "        ids = np.zeros(len([token.text for token in NLP(context)]), dtype ='int')\n",
    "    else:\n",
    "        ids = np.array(ids)\n",
    "        ids =np.sum(ids, axis=0)\n",
    "\n",
    "    for i, id in enumerate(ids):\n",
    "        if id>0:\n",
    "            ids[i]=1\n",
    "        else:\n",
    "            ids[i]=0\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce32076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_ner_ids(sentence, pos_tags_list, ner_tags_list):\n",
    "    doc = NLP(sentence)\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    ner_tags = [token.ent_type_ if token.ent_type_ else 'O' for token in doc]\n",
    "    pos_ids = [pos_tags_list.index(item) if item in pos_tags_list else pos_tags_list.index('X') for item in pos_tags]\n",
    "    ner_ids = [ner_tags_list.index(item) for item in ner_tags]\n",
    "    return pos_ids, ner_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a47c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start get SQuAD raw examples ...\n",
      "Time of get raw examples: 0:00:06.304222\n",
      "Number of raw examples:  86635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is in front of the Notre Dame Main Building?',\n",
       " 'ans_sent': 'Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".',\n",
       " 'answer_text': 'a copper statue of Christ',\n",
       " 'answer_start': 60}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t =get_squad_raw_examples('./Data/train.txt')\n",
    "t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd977eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm này trả về các thông tin như style ids, pos_ids, ner_ids, is_clue\n",
    "def final_data(datas, styles, yes_no_ans, pos_tags_list, ner_tags_list, topN = 20):\n",
    "    style_ids=[]\n",
    "    pos_ids=[]\n",
    "    ner_ids = []\n",
    "    iob_tags = []\n",
    "    is_clue = []\n",
    "    \n",
    "    for i, data in enumerate(datas[0:51000]):\n",
    "        ans_sent = data['ans_sent']\n",
    "        ques = data['question']\n",
    "        ans_text = data['answer_text']\n",
    "        ans_start = data['answer_start']\n",
    "        style_ids.append(list(get_style_ids([ques], styles, yes_no_ans)))\n",
    "        p, n = get_pos_ner_ids(ans_sent, pos_tags_list, ner_tags_list)\n",
    "        pos_ids.append(p)\n",
    "        ner_ids.append(n)\n",
    "        iob = get_iob(ans_sent, ans_text, ans_start)\n",
    "        iob_tags.append(iob)\n",
    "        is_clue.append(get_clue_ids_new(ans_sent, ans_text, iob, 7))\n",
    "    return style_ids, pos_ids, ner_ids, iob_tags, is_clue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bf563af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final = final_data(t, styles, yes_no_ans, pos_tags_list, ner_tags_list)\n",
    "#ĐÂY LÀ HÀM ĐỂ LẤY CÁC THÔNG TIN STYLE IDS, POS_TAG_IDS, NER_TAG_IDS, IS_CLUE\n",
    "# !!!! CHẠY ĐOẠN CODE NÀY MẤT HƠN 1 TIẾNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2025943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('list_ids_data1.pkl', 'wb') as file:\n",
    "#     pickle.dump(final, file)\n",
    "#luu lại các giá trị data đã được xử lý\n",
    "#ĐÃ ĐƯỢC LƯU\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
